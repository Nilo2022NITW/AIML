{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/nqmlGHUF6hY5ovC4xExr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilvalox/AIML/blob/main/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX7yam10uDZB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from plt_overfit import overfit_example, output\n",
        "from lab_utils_common import sigmoid\n",
        "np.set_printoptions(precision=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding regularization\n",
        "\n",
        "The slides above show the cost and gradient functions for both linear and logistic regression. Note:\n",
        "\n",
        "Cost\n",
        "The cost functions differ significantly between linear and logistic regression, but adding regularization to the equations is the same.\n",
        "Gradient\n",
        "The gradient functions for linear and logistic regression are very similar. They differ only in the implementation of  𝑓𝑤𝑏\n",
        " .\n",
        "Cost functions with regularization\n",
        "Cost function for regularized linear regression\n",
        "The equation for the cost function regularized linear regression is:\n",
        "𝐽(𝐰,𝑏)=12𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))2+𝜆2𝑚∑𝑗=0𝑛−1𝑤2𝑗(1)\n",
        "where:\n",
        "𝑓𝐰,𝑏(𝐱(𝑖))=𝐰⋅𝐱(𝑖)+𝑏(2)\n",
        "Compare this to the cost function without regularization (which you implemented in a previous lab), which is of the form:\n",
        "\n",
        "𝐽(𝐰,𝑏)=12𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))2\n",
        "The difference is the regularization term, 𝜆2𝑚∑𝑛−1𝑗=0𝑤2𝑗\n",
        "\n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter 𝑏\n",
        " is not regularized. This is standard practice."
      ],
      "metadata": {
        "id": "wtjfc-NyuNh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "\n",
        "    m  = X.shape[0]\n",
        "    n  = len(w)\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
        "        cost = cost + (f_wb_i - y[i])**2                               #scalar\n",
        "    cost = cost / (2 * m)                                              #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "\n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "metadata": {
        "id": "HwXwPHy1uK8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "metadata": {
        "id": "JJ0SgeZTuVVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function for regularized logistic regression\n",
        "For regularized logistic regression, the cost function is of the form\n",
        "𝐽(𝐰,𝑏)=1𝑚∑𝑖=0𝑚−1[−𝑦(𝑖)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−𝑦(𝑖))log(1−𝑓𝐰,𝑏(𝐱(𝑖)))]+𝜆2𝑚∑𝑗=0𝑛−1𝑤2𝑗(3)\n",
        "where:\n",
        "𝑓𝐰,𝑏(𝐱(𝑖))=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝐰⋅𝐱(𝑖)+𝑏)(4)\n",
        "Compare this to the cost function without regularization (which you implemented in a previous lab):\n",
        "\n",
        "𝐽(𝐰,𝑏)=1𝑚∑𝑖=0𝑚−1[(−𝑦(𝑖)log(𝑓𝐰,𝑏(𝐱(𝑖)))−(1−𝑦(𝑖))log(1−𝑓𝐰,𝑏(𝐱(𝑖)))]\n",
        "\n",
        "As was the case in linear regression above, the difference is the regularization term, which is  𝜆2𝑚∑𝑛−1𝑗=0𝑤2𝑗\n",
        "\n",
        "\n",
        "Including this term encourages gradient descent to minimize the size of the parameters. Note, in this example, the parameter  𝑏\n",
        "  is not regularized. This is standard practice."
      ],
      "metadata": {
        "id": "eqZD8_Sauk3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
        "    \"\"\"\n",
        "    Computes the cost over all examples\n",
        "    Args:\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns:\n",
        "      total_cost (scalar):  cost\n",
        "    \"\"\"\n",
        "\n",
        "    m,n  = X.shape\n",
        "    cost = 0.\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
        "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
        "\n",
        "    cost = cost/m                                                      #scalar\n",
        "\n",
        "    reg_cost = 0\n",
        "    for j in range(n):\n",
        "        reg_cost += (w[j]**2)                                          #scalar\n",
        "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
        "\n",
        "    total_cost = cost + reg_cost                                       #scalar\n",
        "    return total_cost                                                  #scalar"
      ],
      "metadata": {
        "id": "-bhhUb5iuiFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,6)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(\"Regularized cost:\", cost_tmp)"
      ],
      "metadata": {
        "id": "eUOH5jekunIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing the Gradient with regularization (both linear/logistic)\n",
        "The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of  𝑓𝐰𝑏\n",
        " .\n",
        "∂𝐽(𝐰,𝑏)∂𝑤𝑗∂𝐽(𝐰,𝑏)∂𝑏=1𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))𝑥(𝑖)𝑗+𝜆𝑚𝑤𝑗=1𝑚∑𝑖=0𝑚−1(𝑓𝐰,𝑏(𝐱(𝑖))−𝑦(𝑖))(2)(3)\n",
        "m is the number of training examples in the data set\n",
        "𝑓𝐰,𝑏(𝑥(𝑖))\n",
        "  is the model's prediction, while  𝑦(𝑖)\n",
        "  is the target\n",
        "For a linear regression model\n",
        "𝑓𝐰,𝑏(𝑥)=𝐰⋅𝐱+𝑏\n",
        "\n",
        "For a logistic regression model\n",
        "𝑧=𝐰⋅𝐱+𝑏\n",
        "\n",
        "𝑓𝐰,𝑏(𝑥)=𝑔(𝑧)\n",
        "\n",
        "where  𝑔(𝑧)\n",
        "  is the sigmoid function:\n",
        "𝑔(𝑧)=11+𝑒−𝑧\n",
        "\n",
        "The term which adds regularization is the  𝜆𝑚𝑤𝑗\n",
        " ."
      ],
      "metadata": {
        "id": "gXiDT7kFuut6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "\n",
        "    Returns:\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape           #(number of examples, number of features)\n",
        "    dj_dw = np.zeros((n,))\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        err = (np.dot(X[i], w) + b) - y[i]\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
        "        dj_db = dj_db + err\n",
        "    dj_dw = dj_dw / m\n",
        "    dj_db = dj_db / m\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "Do9l2k9duv2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "metadata": {
        "id": "E7pdVjq5u1Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient function for regularized logistic regression"
      ],
      "metadata": {
        "id": "xfNVxo_ou3yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic_reg(X, y, w, b, lambda_):\n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters\n",
        "      b (scalar)      : model parameter\n",
        "      lambda_ (scalar): Controls amount of regularization\n",
        "    Returns\n",
        "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w.\n",
        "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b.\n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    dj_dw = np.zeros((n,))                            #(n,)\n",
        "    dj_db = 0.0                                       #scalar\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "\n",
        "    for j in range(n):\n",
        "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
        "\n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "BpDcDGkwu3j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "X_tmp = np.random.rand(5,3)\n",
        "y_tmp = np.array([0,1,0,1,0])\n",
        "w_tmp = np.random.rand(X_tmp.shape[1])\n",
        "b_tmp = 0.5\n",
        "lambda_tmp = 0.7\n",
        "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
        "\n",
        "print(f\"dj_db: {dj_db_tmp}\", )\n",
        "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
      ],
      "metadata": {
        "id": "lu-Vh1ouu8XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun over-fitting example"
      ],
      "metadata": {
        "id": "PKmv55JQu-_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(\"all\")\n",
        "display(output)\n",
        "ofit = overfit_example(True)"
      ],
      "metadata": {
        "id": "tu1voU8Xu-yG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}