{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCezWDmUVO+7uzJiRi+jYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nilvalox/AIML/blob/main/Logistic_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm1OaeSIwmgN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from plt_logistic_loss import  plt_logistic_cost, plt_two_logistic_loss_curves, plt_simple_example\n",
        "from plt_logistic_loss import soup_bowl, plt_logistic_squared_error\n",
        "plt.style.use('./deeplearning.mplstyle')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall for Linear Regression we have used the squared error cost function: The equation for the squared error cost with one variable is:\n",
        "𝐽(𝑤,𝑏)=12𝑚∑𝑖=0𝑚−1(𝑓𝑤,𝑏(𝑥(𝑖))−𝑦(𝑖))2(1)\n",
        "where\n",
        "𝑓𝑤,𝑏(𝑥(𝑖))=𝑤𝑥(𝑖)+𝑏(2)\n",
        "Recall, the squared error cost had the nice property that following the derivative of the cost leads to the minimum."
      ],
      "metadata": {
        "id": "DvOEeZej0yjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soup_bowl()"
      ],
      "metadata": {
        "id": "24cmYfT40zKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cost function worked well for linear regression, it is natural to consider it for logistic regression as well. However, as the slide above points out,  𝑓𝑤𝑏(𝑥)\n",
        "  now has a non-linear component, the sigmoid function:  𝑓𝑤,𝑏(𝑥(𝑖))=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑤𝑥(𝑖)+𝑏)\n",
        " . Let's try a squared error cost on the example from an earlier lab, now including the sigmoid.\n",
        "\n",
        "Here is our training data:"
      ],
      "metadata": {
        "id": "PRU0BrI30250"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\n",
        "y_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\n",
        "plt_simple_example(x_train, y_train)"
      ],
      "metadata": {
        "id": "1PyiDZJX01VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all')\n",
        "plt_logistic_squared_error(x_train,y_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4XvhE28K05y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_two_logistic_loss_curves()"
      ],
      "metadata": {
        "id": "c2hlbhmX07q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close('all')\n",
        "cst = plt_logistic_cost(x_train,y_train)"
      ],
      "metadata": {
        "id": "jA-RF4BG0-on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost function for logistic regression"
      ],
      "metadata": {
        "id": "H7Ra4szhDyj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "from lab_utils_common import  plot_data, sigmoid, dlc\n",
        "plt.style.use('./deeplearning.mplstyle')"
      ],
      "metadata": {
        "id": "9nptvnEa1Aqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])"
      ],
      "metadata": {
        "id": "Dqq2NBsGD3iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
        "plot_data(X_train, y_train, ax)\n",
        "\n",
        "# Set both axes to be from 0-4\n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "liOMph0bD5q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Description\n",
        "The algorithm for compute_cost_logistic loops over all the examples calculating the loss for each example and accumulating the total.\n",
        "\n",
        "Note that the variables X and y are not scalar values but matrices of shape ( 𝑚,𝑛\n",
        " ) and ( 𝑚\n",
        " ,) respectively, where  𝑛\n",
        "  is the number of features and  𝑚\n",
        "  is the number of training examples."
      ],
      "metadata": {
        "id": "-gWdPbOZD_iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes cost\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters\n",
        "      b (scalar)       : model parameter\n",
        "\n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i],w) + b\n",
        "        f_wb_i = sigmoid(z_i)\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "\n",
        "    cost = cost / m\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "kiFO-zb8ECiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp = np.array([1,1])\n",
        "b_tmp = -3\n",
        "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
      ],
      "metadata": {
        "id": "WewHquGuEHvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose values between 0 and 6\n",
        "x0 = np.arange(0,6)\n",
        "\n",
        "# Plot the two decision boundaries\n",
        "x1 = 3 - x0\n",
        "x1_other = 4 - x0\n",
        "\n",
        "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "# Plot the decision boundary\n",
        "ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\n",
        "ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\n",
        "ax.axis([0, 4, 0, 4])\n",
        "\n",
        "# Plot the original data\n",
        "plot_data(X_train,y_train,ax)\n",
        "ax.axis([0, 4, 0, 4])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Decision Boundary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjyxWi7rEUpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_array1 = np.array([1,1])\n",
        "b_1 = -3\n",
        "w_array2 = np.array([1,1])\n",
        "b_2 = -4\n",
        "\n",
        "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
        "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
      ],
      "metadata": {
        "id": "G1XyNF5PEQsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}